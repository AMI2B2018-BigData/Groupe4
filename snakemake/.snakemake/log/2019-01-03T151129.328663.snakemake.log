Building DAG of jobs...
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	all
	1	cornell
	1	sort
	3

rule cornell:
    input: ../python/confs_withH/1BRS_A_1BRS_B_allatom_939_DP.pdb
    output: resultat.txt
    jobid: 2

Error in rule cornell:
    jobid: 2
    output: resultat.txt

RuleException:
CalledProcessError in line 51 of /home/martin/Documents/M2-AMI2B/bigdata/Workflow-ProjectV2/snakemake/Snakefile:
Command ' set -euo pipefail;  pypy cornell.py -rec ../python/Rec_natif.pdb -lig ../python/confs_withH/1BRS_A_1BRS_B_allatom_939_DP.pdb >> resultat.txt ' returned non-zero exit status 1.
  File "/home/martin/Documents/M2-AMI2B/bigdata/Workflow-ProjectV2/snakemake/Snakefile", line 51, in __rule_cornell
  File "/usr/lib/python3.6/concurrent/futures/thread.py", line 56, in run
Removing output files of failed job cornell since they might be corrupted:
resultat.txt
Will exit after finishing currently running jobs.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2019-01-03T151129.328663.snakemake.log
