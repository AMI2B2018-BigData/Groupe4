Building DAG of jobs...
Provided cores: 4
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	all
	1	merge_cor
	1	merge_hyd
	3

rule merge_cor:
    input: Cornell_results/
    output: Cornell_results.txt
    jobid: 1694


rule merge_hyd:
    input: Hydrophobic_results
    output: Hydroph_results.txt
    jobid: 454

Error in rule merge_hyd:
    jobid: 454
    output: Hydroph_results.txt

RuleException:
CalledProcessError in line 83 of /home/martin/Documents/M2-AMI2B/bigdata/Workflow-ProjectV2/snakemake/Snakefile:
Command ' set -euo pipefail;  paste -d "
" Hydrophobic_results* > Hydroph_results.txt ' returned non-zero exit status 1.
  File "/home/martin/Documents/M2-AMI2B/bigdata/Workflow-ProjectV2/snakemake/Snakefile", line 83, in __rule_merge_hyd
  File "/usr/lib/python3.6/concurrent/futures/thread.py", line 56, in run
Removing output files of failed job merge_hyd since they might be corrupted:
Hydroph_results.txt
Will exit after finishing currently running jobs.
Finished job 1694.
1 of 3 steps (33%) done
Will exit after finishing currently running jobs.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2019-01-06T012656.052981.snakemake.log
