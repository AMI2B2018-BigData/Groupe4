Building DAG of jobs...
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	all
	948	cornell
	1	merge_sort
	950

rule cornell:
    input: ../python/confs_withH/1BRS_A_1BRS_B_allatom_220_DP.pdb
    output: Cornell_results/1BRS_A_1BRS_B_allatom_220_DP.pdb_cor.txt
    jobid: 384
    wildcards: lig=1BRS_A_1BRS_B_allatom_220_DP.pdb

Error in rule cornell:
    jobid: 384
    output: Cornell_results/1BRS_A_1BRS_B_allatom_220_DP.pdb_cor.txt

RuleException:
CalledProcessError in line 48 of /home/martin/Documents/M2-AMI2B/bigdata/Workflow-ProjectV2/snakemake/Snakefile:
Command ' set -euo pipefail;  pypy cornell.py -rec ../python/Rec_natif.pdb -lig ../python/confs_withH/1BRS_A_1BRS_B_allatom_220_DP.pdb > Cornell_results/1BRS_A_1BRS_B_allatom_220_DP.pdb_cor.txt ' returned non-zero exit status 1.
  File "/home/martin/Documents/M2-AMI2B/bigdata/Workflow-ProjectV2/snakemake/Snakefile", line 48, in __rule_cornell
  File "/usr/lib/python3.6/concurrent/futures/thread.py", line 56, in run
Removing output files of failed job cornell since they might be corrupted:
Cornell_results/1BRS_A_1BRS_B_allatom_220_DP.pdb_cor.txt
Will exit after finishing currently running jobs.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2019-01-05T153047.712272.snakemake.log
